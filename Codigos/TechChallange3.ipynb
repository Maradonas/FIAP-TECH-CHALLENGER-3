{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d6ca84",
   "metadata": {},
   "source": [
    "#### **Fundamentos de Bancos de Dados Relacionais e NoSQL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59115f",
   "metadata": {},
   "source": [
    "#### **Conteúdo - Bases e Notebook da aula**\n",
    "\n",
    "https://github.com/FIAP/Pos_Tech_DTAT/tree/main/Fase%203"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770d68e",
   "metadata": {},
   "source": [
    "#### **Importação de pacotes, bibliotecas e funções (def)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23ccd403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: botocore 1.40.26\n",
      "Uninstalling botocore-1.40.26:\n",
      "  Successfully uninstalled botocore-1.40.26\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall boto3 -y\n",
    "!pip uninstall botocore -y\n",
    "# !pip install boto3==1.40.26 \n",
    "# !pip install botocore==1.40.26\n",
    "# # !pip install s3fs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48bccaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting botocore==1.40.26\n",
      "  Using cached botocore-1.40.26-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\dmaradini\\appdata\\local\\anaconda3\\lib\\site-packages (from botocore==1.40.26) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\dmaradini\\appdata\\local\\anaconda3\\lib\\site-packages (from botocore==1.40.26) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\dmaradini\\appdata\\local\\anaconda3\\lib\\site-packages (from botocore==1.40.26) (2.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dmaradini\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.40.26) (1.16.0)\n",
      "Using cached botocore-1.40.26-py3-none-any.whl (14.0 MB)\n",
      "Installing collected packages: botocore\n",
      "Successfully installed botocore-1.40.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.12.3 requires botocore<1.34.70,>=1.34.41, but you have botocore 1.40.26 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "#!pip install boto3==1.40.26 \n",
    "!pip install botocore==1.40.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "691abc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3: 1.40.26\n",
      "botocore: 1.40.26\n"
     ]
    }
   ],
   "source": [
    "import boto3, botocore\n",
    "print(\"boto3:\", boto3.__version__)\n",
    "print(\"botocore:\", botocore.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ca788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar biblioteca completa\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import botocore\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import mongomock\n",
    "import fakeredis\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "# Importar função especifica de um módulo\n",
    "from botocore.exceptions import BotoCoreError, ClientError\n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "from dotenv import load_dotenv\n",
    "from io import StringIO,BytesIO\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from astrapy import DataAPIClient\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f092b",
   "metadata": {},
   "source": [
    "#### **Testar conexão AWS via Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02baef19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conectado à conta\n",
      "\n",
      "UserId: AROAZI2LFHB2TERRDYNA4:user4377774=maradinidiego16@gmail.com\n",
      "Account: 637423401077\n",
      "Arn: arn:aws:sts::637423401077:assumed-role/voclabs/user4377774=maradinidiego16@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# Validar conexão\n",
    "try:\n",
    "    session = boto3.Session(profile_name=\"default\")\n",
    "    sts = session.client(\"sts\")\n",
    "    identity = sts.get_caller_identity()\n",
    "    print(\"✅ Conectado à conta\\n\")\n",
    "    print(\"UserId:\", identity[\"UserId\"])\n",
    "    print(\"Account:\", identity[\"Account\"])\n",
    "    print(\"Arn:\", identity[\"Arn\"])\n",
    "\n",
    "except (BotoCoreError, ClientError) as e:\n",
    "    print(\"❌ Erro ao conectar à AWS. Verifique suas credenciais e tente novamente.\")\n",
    "    print(\"Detalhes do erro:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d94c24",
   "metadata": {},
   "source": [
    "##### **Conectar ao PostgreSQL via RDS + Executar Comandos SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cc352e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\dmaradini\\AppData\\Local\\Temp\\ipykernel_18620\\2708774549.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  load_dotenv(\"Users\\dmaradini\\.aws\\credentials\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv(\"Users\\dmaradini\\.aws\\credentials\")\n",
    "\n",
    "aws_access_key_id       = os.getenv(\"aws_access_key_id\")\n",
    "aws_secret_access_key   = os.getenv(\"aws_secret_access_key\")\n",
    "aws_session_token       = os.getenv(\"aws_session_token\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ae193b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'techchallange-637423401077' já existe e está acessível.\n",
      "Bucket 'techchallange-637423401077' configurado para leitura pública via Bucket Policy.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_session_token\n",
    ")\n",
    "\n",
    "bucket_name = \"techchallange-637423401077\"\n",
    "s3 = session.client(\"s3\")\n",
    "\n",
    "# Descobrir região da sessão; se não houver, use us-east-1\n",
    "region = session.region_name or s3.meta.region_name or \"us-east-1\"\n",
    "\n",
    "# 1) Verificar se o bucket existe e é seu/acessível\n",
    "def bucket_exists_and_accessible(name: str) -> bool:\n",
    "    try:\n",
    "        s3.head_bucket(Bucket=name)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        code = e.response.get(\"Error\", {}).get(\"Code\", \"\")\n",
    "        # 404/NoSuchBucket => não existe; 301 => região diferente; 403 => existe mas sem acesso\n",
    "        if code in (\"404\", \"NoSuchBucket\"):\n",
    "            return False\n",
    "        elif code in (\"301\", \"PermanentRedirect\"):\n",
    "            # Existe em outra região; ainda dá para usar, mas crie/use cliente na região correta\n",
    "            return True\n",
    "        elif code in (\"403\", \"AccessDenied\"):\n",
    "            # Bucket provavelmente existe mas não é seu (nome global do S3)\n",
    "            raise RuntimeError(\n",
    "                f\"Bucket '{name}' já existe, mas você não tem acesso. \"\n",
    "                \"Escolha outro nome (o namespace do S3 é global).\"\n",
    "            )\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# 2) Criar bucket SEM ACLs (compatível com ObjectOwnership: BucketOwnerEnforced)\n",
    "if bucket_exists_and_accessible(bucket_name):\n",
    "    print(f\"Bucket '{bucket_name}' já existe e está acessível.\")\n",
    "else:\n",
    "    print(f\"Bucket '{bucket_name}' não existe; criando...\")\n",
    "    if region == \"us-east-1\":\n",
    "        # Em us-east-1 não envia CreateBucketConfiguration\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    else:\n",
    "        s3.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={\"LocationConstraint\": region},\n",
    "        )\n",
    "    print(f\"Bucket '{bucket_name}' criado com sucesso.\")\n",
    "\n",
    "# 3) Ajustar Public Access Block:\n",
    "#    Para permitir bucket policy pública, desative os bloqueios de policy no BUCKET.\n",
    "#    (Os bloqueios de ACL podem continuar como True, já que não usamos ACLs.)\n",
    "s3.put_public_access_block(\n",
    "    Bucket=bucket_name,\n",
    "    PublicAccessBlockConfiguration={\n",
    "        \"BlockPublicAcls\": True,      # pode ficar True (ACLs desativadas por padrão)\n",
    "        \"IgnorePublicAcls\": True,     # idem\n",
    "        \"BlockPublicPolicy\": False,   # precisa ser False para aceitar policy pública\n",
    "        \"RestrictPublicBuckets\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "# 4) Aplicar bucket policy pública de leitura de objetos\n",
    "public_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"AllowPublicReadOfObjects\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": \"*\",\n",
    "            \"Action\": [\"s3:GetObject\"],\n",
    "            \"Resource\": f\"arn:aws:s3:::{bucket_name}/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(public_policy))\n",
    "print(f\"Bucket '{bucket_name}' configurado para leitura pública via Bucket Policy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00071434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexão estabelecida.\n",
      "Bucket 'techchallange-637423401077' já existe e está acessível.\n",
      "Bucket 'techchallange-637423401077' configurado como público.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import numpy as npfrp\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id       =   aws_access_key_id\n",
    "    ,aws_secret_access_key  =   aws_secret_access_key\n",
    "    ,aws_session_token      =   aws_session_token\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "bucket_name = \"techchallange-637423401077\"\n",
    "s3_prefix = \"raw\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Conexão estabelecida.\")\n",
    "\n",
    "s3 = session.client('s3')\n",
    "region = s3.meta.region_name or \"us-east-1\"\n",
    "\n",
    "try:\n",
    "    s3.head_bucket(Bucket=bucket_name)\n",
    "    print(f\"Bucket '{bucket_name}' já existe e está acessível.\")\n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    if error_code in (\"404\", \"NoSuchBucket\"):\n",
    "        print(f\"Bucket '{bucket_name}' não existe, criando...\\n\")\n",
    "        if region == \"us-east-1\":\n",
    "            s3.create_bucket(Bucket=bucket_name)\n",
    "            \n",
    "        else:\n",
    "            s3.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f\"Bucket '{bucket_name}' criado com sucesso.\\n\")\n",
    "    else:\n",
    "        print(f\"Erro ao acessar o bucket: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "# Criar política de bucket pública\n",
    "public_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PublicReadGetObject\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": \"*\",\n",
    "            \"Action\": \"s3:GetObject\",\n",
    "            \"Resource\": f\"arn:aws:s3:::{bucket_name}/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(public_policy))\n",
    "print(f\"Bucket '{bucket_name}' configurado como público.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2254431",
   "metadata": {},
   "source": [
    "### CAMADA BRONZE (EXTRACT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28b14473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando PNAD_COVID_052020...\n",
      "Dados salvos no S3 em s3://techchallange-637423401077/raw/PNAD_COVID_052020.csv\n",
      "Processando PNAD_COVID_062020...\n",
      "Dados salvos no S3 em s3://techchallange-637423401077/raw/PNAD_COVID_062020.csv\n",
      "Processando PNAD_COVID_072020...\n",
      "Dados salvos no S3 em s3://techchallange-637423401077/raw/PNAD_COVID_072020.csv\n",
      "Exportação concluída com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from io import BytesIO, StringIO\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "\n",
    "bucket_name = \"techchallange-637423401077\"\n",
    "s3_prefix = \"raw\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# URLs dos arquivos do IBGE\n",
    "urls = {\n",
    "    \"PNAD_COVID_052020\": \"https://ftp.ibge.gov.br/Trabalho_e_Rendimento/Pesquisa_Nacional_por_Amostra_de_Domicilios_PNAD_COVID19/Microdados/Dados/PNAD_COVID_052020.zip\",\n",
    "    \"PNAD_COVID_062020\": \"https://ftp.ibge.gov.br/Trabalho_e_Rendimento/Pesquisa_Nacional_por_Amostra_de_Domicilios_PNAD_COVID19/Microdados/Dados/PNAD_COVID_062020.zip\",\n",
    "    \"PNAD_COVID_072020\": \"https://ftp.ibge.gov.br/Trabalho_e_Rendimento/Pesquisa_Nacional_por_Amostra_de_Domicilios_PNAD_COVID19/Microdados/Dados/PNAD_COVID_072020.zip\",\n",
    "}\n",
    "\n",
    "for tabela, url in urls.items():\n",
    "    print(f\"Processando {tabela}...\")\n",
    "\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    if \"Dicionario\" in tabela:\n",
    "        # Salvar o XLS original no S3\n",
    "        s3_key = f\"{s3_prefix}/{tabela}.xls\"\n",
    "        s3.put_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=s3_key,\n",
    "            Body=r.content  # conteúdo binário do XLS\n",
    "        )\n",
    "        print(f\"Dicionário salvo no S3 em s3://{bucket_name}/{s3_key}\")\n",
    "\n",
    "    else:\n",
    "        # Abrir o ZIP e extrair o CSV\n",
    "        with ZipFile(BytesIO(r.content)) as z:\n",
    "            csv_name = z.namelist()[0]\n",
    "            with z.open(csv_name) as f:\n",
    "                df = pd.read_csv(f, sep=\";\", encoding=\"latin1\")\n",
    "\n",
    "        # Converter para CSV em memória\n",
    "        csv_buffer = StringIO()\n",
    "        df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "        s3_key = f\"{s3_prefix}/{tabela}.csv\"\n",
    "        s3.put_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=s3_key,\n",
    "            Body=csv_buffer.getvalue()\n",
    "        )\n",
    "        print(f\"Dados salvos no S3 em s3://{bucket_name}/{s3_key}\")\n",
    "\n",
    "print(\"Exportação concluída com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a90a04a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consegui conectar no S3!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name=\"default\")\n",
    "s3 = session.client(\"s3\")\n",
    "print(\"Consegui conectar no S3!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdffda6",
   "metadata": {},
   "source": [
    "### CAMADA SILVER (TRANFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca9b2f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo https://techchallange-637423401077.s3.us-east-1.amazonaws.com/raw/PNAD_COVID_052020.csv\n",
      "Lendo https://techchallange-637423401077.s3.us-east-1.amazonaws.com/raw/PNAD_COVID_062020.csv\n",
      "Lendo https://techchallange-637423401077.s3.us-east-1.amazonaws.com/raw/PNAD_COVID_072020.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Lista de arquivos CSV no bucket público\n",
    "bronze_files = [\n",
    "    \"PNAD_COVID_052020.csv\",\n",
    "    \"PNAD_COVID_062020.csv\",\n",
    "    \"PNAD_COVID_072020.csv\"\n",
    "]\n",
    "\n",
    "# URL base do bucket público\n",
    "base_url = \"https://techchallange-637423401077.s3.us-east-1.amazonaws.com/raw/\"\n",
    "\n",
    "lista_de_dataframes = []\n",
    "\n",
    "for file_name in bronze_files:\n",
    "    file_url = f\"{base_url}{file_name}\"\n",
    "    print(f\"Lendo {file_url}\")\n",
    "    \n",
    "    # Faz download do CSV como texto\n",
    "    response = requests.get(file_url)\n",
    "    response.raise_for_status()  # garante que deu certo\n",
    "    \n",
    "    # Lê o CSV como texto cru\n",
    "    df_raw = pd.read_csv(StringIO(response.text), header=None)\n",
    "    \n",
    "    # Divide a única coluna pelo separador vírgula\n",
    "    df_temp = df_raw[0].str.split(\",\", expand=True)\n",
    "    \n",
    "    # Define a primeira linha como header\n",
    "    df_temp.columns = df_temp.iloc[0]\n",
    "    df_temp = df_temp.drop(0).reset_index(drop=True)\n",
    "    \n",
    "    lista_de_dataframes.append(df_temp)\n",
    "\n",
    "# Consolidar todos os dataframes\n",
    "df_consolidado = pd.concat(lista_de_dataframes, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "039dae51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame salvo em s3://techchallange-637423401077/silver/df_consolidado.parquet\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# Configurações do bucket\n",
    "bucket_name = \"techchallange-637423401077\"\n",
    "s3_key = \"silver/df_consolidado.parquet\"\n",
    "\n",
    "# Conexão S3\n",
    "s3_client = boto3.client(\"s3\")  # precisa das credenciais configuradas no seu ambiente\n",
    "\n",
    "# Salva o DataFrame em um buffer Parquet\n",
    "buffer = BytesIO()\n",
    "df_consolidado.to_parquet(buffer, index=False)\n",
    "\n",
    "# Envia para o S3\n",
    "buffer.seek(0)\n",
    "s3_client.put_object(Bucket=bucket_name, Key=s3_key, Body=buffer.getvalue())\n",
    "\n",
    "print(f\"DataFrame salvo em s3://{bucket_name}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db6a0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consolidado_parquet_aws = pd.read_parquet(\"https://techchallange-637423401077.s3.us-east-1.amazonaws.com/silver/df_consolidado.parquet\")\n",
    "#pd.read_parquet('Dados.parquet') #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff64cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consolidado_parquet = df_consolidado_parquet_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7776874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dicionário de renomeação completo\n",
    "mapa_colunas = {\n",
    "    # Colunas fixas\n",
    "    \"ANO\": \"ano\",\n",
    "    \"V1013\": \"mes_pesquisa\",\n",
    "    \"V1012\": \"semana_mes\",\n",
    "    \"UF\": \"uf\",\n",
    "    \"CAPITAL\": \"capital\",\n",
    "\n",
    "    # Colunas desejadas\n",
    "    \"A002\": \"idade\",\n",
    "    \"A003\": \"sexo\",\n",
    "    \"A004\": \"raca_cor\",\n",
    "    \"A006B\": \"aulas_presenciais\",\n",
    "    \"B008\": \"fez_exame\",\n",
    "    \"B009A\": \"exame_swab\",\n",
    "    \"B009C\": \"exame_furo_dedo\",\n",
    "    \"B009E\": \"exame_veia_braco\",\n",
    "    \"A005\": \"escolaridade\",\n",
    "    \"A006\": \"frequenta_escola\",\n",
    "    \"A006A\": \"tipo_escola\",\n",
    "    \"B0011\": \"teve_febre_semana_passada\",\n",
    "    \"B0012\": \"teve_tosse_semana_passada\",\n",
    "    \"B0013\": \"teve_dor_garganta_semana_passada\",\n",
    "    \"B0014\": \"teve_dificuldade_respirar\",\n",
    "    \"B0015\": \"teve_dor_cabeca\",\n",
    "    \"B0016\": \"teve_dor_peito\",\n",
    "    \"B0017\": \"teve_nausea\",\n",
    "    \"B0018\": \"teve_nariz_entupido\",\n",
    "    \"B0019\": \"teve_fadiga\",\n",
    "    \"B00110\": \"teve_dor_olhos\",\n",
    "    \"B00111\": \"teve_perda_cheiro_sabor\",\n",
    "    \"B00112\": \"teve_dor_muscular\",\n",
    "    \"B00113\": \"teve_diarreia\"\n",
    "}\n",
    "\n",
    "\n",
    "colunas_existentes = [col for col in mapa_colunas.keys() if col in df_consolidado_parquet.columns]\n",
    "\n",
    "# Renomeia apenas essas colunas\n",
    "df_consolidado_parquet.rename(columns={col: mapa_colunas[col] for col in colunas_existentes}, inplace=True)\n",
    "\n",
    "\n",
    "df_consolidado_parquet = df_consolidado_parquet[[mapa_colunas[col] for col in colunas_existentes]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa_sim_nao = {\n",
    "    1: \"Sim\",\n",
    "    2: \"Não\",\n",
    "    3: \"Não sabe\",\n",
    "    9: \"Ignorado\"\n",
    "}\n",
    "\n",
    "mapa_uf = {\n",
    "    11: \"Rondônia\", 12: \"Acre\", 13: \"Amazonas\", 14: \"Roraima\", 15: \"Pará\", 16: \"Amapá\", 17: \"Tocantins\",\n",
    "    21: \"Maranhão\", 22: \"Piauí\", 23: \"Ceará\", 24: \"Rio Grande do Norte\", 25: \"Paraíba\", 26: \"Pernambuco\",\n",
    "    27: \"Alagoas\", 28: \"Sergipe\", 29: \"Bahia\", 31: \"Minas Gerais\", 32: \"Espírito Santo\", 33: \"Rio de Janeiro\",\n",
    "    35: \"São Paulo\", 41: \"Paraná\", 42: \"Santa Catarina\", 43: \"Rio Grande do Sul\", 50: \"Mato Grosso do Sul\",\n",
    "    51: \"Mato Grosso\", 52: \"Goiás\", 53: \"Distrito Federal\"\n",
    "}\n",
    "\n",
    "\n",
    "df_consolidado_parquet = df_consolidado_parquet.copy()\n",
    "\n",
    "df_consolidado_parquet[\"uf\"] = df_consolidado_parquet[\"uf\"].astype(int)\n",
    "df_consolidado_parquet[\"uf\"] = df_consolidado_parquet[\"uf\"].replace(mapa_uf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Substituir colunas de Sim/Não (exemplo: febre, tosse, exames, etc)\n",
    "colunas_sim_nao = [\n",
    "    \"fez_exame\", \"exame_swab\", \"exame_furo_dedo\", \"exame_veia_braco\",\n",
    "    \"teve_febre_semana_passada\", \"teve_tosse_semana_passada\", \"teve_dor_garganta_semana_passada\",\n",
    "    \"teve_dificuldade_respirar\", \"teve_dor_cabeca\", \"teve_dor_peito\", \"teve_nausea\",\n",
    "    \"teve_nariz_entupido\", \"teve_fadiga\", \"teve_dor_olhos\", \"teve_perda_cheiro_sabor\",\n",
    "    \"teve_dor_muscular\", \"teve_diarreia\", \"aulas_presenciais\", \"frequenta_escola\"\n",
    "]\n",
    "colunas_sim_nao = [\n",
    "    \"fez_exame\", \"exame_swab\", \"exame_furo_dedo\", \"exame_veia_braco\",\n",
    "    \"teve_febre_semana_passada\", \"teve_tosse_semana_passada\", \"teve_dor_garganta_semana_passada\",\n",
    "    \"teve_dificuldade_respirar\", \"teve_dor_cabeca\", \"teve_dor_peito\", \"teve_nausea\",\n",
    "    \"teve_nariz_entupido\", \"teve_fadiga\", \"teve_dor_olhos\", \"teve_perda_cheiro_sabor\",\n",
    "    \"teve_dor_muscular\", \"teve_diarreia\", \"aulas_presenciais\", \"frequenta_escola\"\n",
    "]\n",
    "\n",
    "# Criar um dicionário de substituição com strings\n",
    "mapa_sim_nao_str = {str(k): v for k, v in mapa_sim_nao.items()}\n",
    "\n",
    "for col in colunas_sim_nao:\n",
    "    if col in df_consolidado_parquet.columns:\n",
    "        # Substituir valores apenas se não forem nulos\n",
    "        df_consolidado_parquet[col] = df_consolidado_parquet[col].apply(\n",
    "            lambda x: mapa_sim_nao_str.get(str(x), x) if pd.notna(x) else x\n",
    "        )\n",
    "\n",
    "# Dicionário de substituição\n",
    "mapa_sexo = {\n",
    "    \"1\": \"Homem\",\n",
    "    \"2\": \"Mulher\"\n",
    "}\n",
    "\n",
    "# Substituir valores na coluna 'sexo' (somente se a coluna existir)\n",
    "if \"sexo\" in df_consolidado_parquet.columns:\n",
    "    df_consolidado_parquet[\"sexo\"] = df_consolidado_parquet[\"sexo\"].astype(str).replace(mapa_sexo)\n",
    "\n",
    "\n",
    "# Dicionário de mapeamento para raca_cor\n",
    "mapa_raca_cor = {\n",
    "    \"1\": \"Branca\",\n",
    "    \"2\": \"Preta\",\n",
    "    \"3\": \"Amarela\",\n",
    "    \"4\": \"Parda\",\n",
    "    \"5\": \"Indígena\",\n",
    "    \"9\": \"Ignorado\"\n",
    "}\n",
    "\n",
    "# Aplicar mapeamento no dataframe\n",
    "if \"raca_cor\" in df_consolidado_parquet.columns:\n",
    "    df_consolidado_parquet[\"raca_cor\"] = df_consolidado_parquet[\"raca_cor\"].astype(str).replace(mapa_raca_cor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11897196",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consolidado_parquet[\"sexo\"] = df_consolidado_parquet[\"sexo\"].astype(\"category\")\n",
    "df_consolidado_parquet[\"mes_pesquisa\"] = df_consolidado_parquet[\"mes_pesquisa\"].astype(\"category\")\n",
    "df_consolidado_parquet[\"raca_cor\"] = df_consolidado_parquet[\"raca_cor\"].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47886e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame salvo em s3://techchallange-637423401077/silver/df_consolidado_Tratado.parquet\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# Seu DataFrame\n",
    "# df_consolidado\n",
    "\n",
    "# Configurações do bucket\n",
    "bucket_name = \"techchallange-637423401077\"\n",
    "s3_key = \"silver/df_consolidado_Tratado.parquet\"\n",
    "\n",
    "# Conexão S3\n",
    "s3_client = boto3.client(\"s3\")  # precisa das credenciais configuradas no seu ambiente\n",
    "\n",
    "# Salva o DataFrame em um buffer Parquet\n",
    "buffer = BytesIO()\n",
    "df_consolidado_parquet.to_parquet(buffer, index=False)\n",
    "\n",
    "# Envia para o S3\n",
    "buffer.seek(0)\n",
    "s3_client.put_object(Bucket=bucket_name, Key=s3_key, Body=buffer.getvalue())\n",
    "\n",
    "print(f\"DataFrame salvo em s3://{bucket_name}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eb1505",
   "metadata": {},
   "source": [
    "### CAMADA OURO (LOAD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c47b6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] s3://techchallange-637423401077/gold/base_curada.parquet (1,114,733 linhas)\n",
      "[OK] s3://techchallange-637423401077/gold/pop_sexo.parquet (2 linhas)\n",
      "[OK] s3://techchallange-637423401077/gold/pop_faixa_etaria.parquet (8 linhas)\n",
      "[OK] s3://techchallange-637423401077/gold/pop_raca_cor.parquet (6 linhas)\n",
      "[OK] s3://techchallange-637423401077/gold/pop_top10_uf.parquet (10 linhas)\n",
      "[OK] s3://techchallange-637423401077/gold/pop_sexo_por_uf.parquet (20 linhas)\n",
      "[OK] s3://techchallange-637423401077/gold/cli_prevalencia_sintomas.parquet (13 linhas)\n",
      "[OK] s3://techchallange-637423401077/gold/cli_n_sintomas_hist.parquet (14 linhas)\n",
      "[OK] s3://techchallange-637423401077/gold/cli_coocorrencia_spearman_wide.parquet (13 linhas)\n",
      "[OK] s3://techchallange-637423401077/gold/cli_tosse_contagem_por_sexo.parquet (2 linhas)\n",
      "[OK] s3://techchallange-637423401077/gold/cli_tosse_percentual_por_sexo.parquet (2 linhas)\n",
      "[OK] s3://techchallange-637423401077/gold/cli_febre_contagem_por_sexo.parquet (2 linhas)\n",
      "[OK] s3://techchallange-637423401077/gold/cli_febre_percentual_por_sexo.parquet (2 linhas)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmaradini\\AppData\\Local\\Temp\\ipykernel_18620\\2031992308.py:239: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  trend = (tmp.groupby(time_col)[[\"sx_febre\",\"sx_tosse\",\"sx_cabeca\"]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] s3://techchallange-637423401077/gold/tmp_tendencia_sintomas.parquet (9 linhas)\n",
      "\n",
      "[ETL] Concluído. Entrada via URL e agregados em s3://techchallange-637423401077/gold/\n"
     ]
    }
   ],
   "source": [
    "INPUT_PATH = \"https://techchallange-637423401077.s3.us-east-1.amazonaws.com/silver/df_consolidado_Tratado.parquet\"\n",
    "\n",
    "# S3 de DESTINO (GOLD)\n",
    "bucket_name = \"techchallange-637423401077\"\n",
    "prefix = \"gold/\"  # \"pasta\" no bucket (terminar com /)\n",
    "\n",
    "SINTOMA_PREFIX = \"teve_\"  # colunas de sintomas começam com isso\n",
    "\n",
    "# =========================\n",
    "# S3 helpers\n",
    "# =========================\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "def save_parquet_s3(df: pd.DataFrame, key: str):\n",
    "    \"\"\"\n",
    "    Salva um DataFrame em formato Parquet no S3 (sem passar por disco),\n",
    "    no bucket/prefix configurados acima.\n",
    "    \"\"\"\n",
    "    buf = BytesIO()\n",
    "    df.to_parquet(buf, index=False, engine=\"pyarrow\")\n",
    "    buf.seek(0)\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=key, Body=buf.getvalue())\n",
    "    print(f\"[OK] s3://{bucket_name}/{key} ({len(df):,} linhas)\")\n",
    "\n",
    "# =========================\n",
    "# IO (entrada) - URL HTTP e fallback boto3\n",
    "# =========================\n",
    "def _read_parquet_http(url: str) -> pd.DataFrame:\n",
    "    \"\"\"Lê um .parquet via HTTP(S). Se falhar e for URL S3, tenta fallback via boto3.get_object().\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=120)\n",
    "        r.raise_for_status()\n",
    "        return pd.read_parquet(BytesIO(r.content), engine=\"pyarrow\")\n",
    "    except Exception as e_http:\n",
    "        # Fallback: tenta boto3 se a URL for do S3 (objeto privado, por ex.)\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            # Ex.: techchallange-...s3.us-east-1.amazonaws.com/silver/df.parquet\n",
    "            key = parsed.path.lstrip(\"/\")                # silver/df_consolidado_Tratado.parquet\n",
    "            bucket = parsed.netloc.split(\".\")[0]         # techchallange-637423401077\n",
    "            obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "            body = obj[\"Body\"].read()\n",
    "            return pd.read_parquet(BytesIO(body), engine=\"pyarrow\")\n",
    "        except Exception as e_s3:\n",
    "            raise RuntimeError(f\"Falha ao ler {url} (HTTP e fallback S3). HTTP: {e_http} | S3: {e_s3}\")\n",
    "\n",
    "def read_any(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Lê CSV/Parquet local, diretório com vários Parquets, OU URL HTTP(S).\"\"\"\n",
    "    if path.lower().startswith((\"http://\", \"https://\")):\n",
    "        if path.lower().endswith(\".parquet\"):\n",
    "            return _read_parquet_http(path)\n",
    "        elif path.lower().endswith(\".csv\"):\n",
    "            r = requests.get(path, timeout=120)\n",
    "            r.raise_for_status()\n",
    "            return pd.read_csv(BytesIO(r.content), sep=\";\", low_memory=False)\n",
    "        else:\n",
    "            raise ValueError(\"URL deve apontar para .parquet ou .csv\")\n",
    "    # Local (opcional)\n",
    "    p = Path(path)\n",
    "    if p.is_dir():\n",
    "        files = [str(x) for x in p.rglob(\"*.parquet\")]\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"Nenhum .parquet em {p}\")\n",
    "        dfs = [pd.read_parquet(f, engine=\"pyarrow\") for f in files]\n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "    suf = p.suffix.lower()\n",
    "    if suf == \".csv\":\n",
    "        return pd.read_csv(p, sep=\";\", low_memory=False)\n",
    "    if suf == \".parquet\":\n",
    "        return pd.read_parquet(p, engine=\"pyarrow\")\n",
    "    raise ValueError(\"Forneça .csv, .parquet, diretório contendo .parquet, ou URL http(s) para .parquet/.csv\")\n",
    "\n",
    "df = read_any(INPUT_PATH)\n",
    "\n",
    "# =========================\n",
    "# NORMALIZAÇÕES\n",
    "# =========================\n",
    "def to_bool_series(s: pd.Series) -> pd.Series:\n",
    "    if s is None:\n",
    "        return None\n",
    "    if s.dtype == bool:\n",
    "        return s.astype(\"boolean\")\n",
    "    t = s.astype(str).str.strip().str.lower()\n",
    "    true_vals = {\"1\",\"true\",\"t\",\"sim\",\"s\",\"y\",\"yes\"}\n",
    "    false_vals = {\"0\",\"false\",\"f\",\"nao\",\"não\",\"n\",\"no\"}\n",
    "    out = pd.Series(pd.NA, index=s.index, dtype=\"boolean\")\n",
    "    out = out.mask(t.isin(true_vals), True).mask(t.isin(false_vals), False)\n",
    "    return out\n",
    "\n",
    "# Idade\n",
    "if \"idade\" in df.columns:\n",
    "    df[\"idade\"] = pd.to_numeric(df[\"idade\"], errors=\"coerce\")\n",
    "    df = df[(df[\"idade\"].notna()) & (df[\"idade\"] >= 0) & (df[\"idade\"] <= 110)]\n",
    "\n",
    "# Sexo\n",
    "if \"sexo\" in df.columns:\n",
    "    df[\"sexo\"] = df[\"sexo\"].astype(str).str.strip().str.title()\n",
    "    mapa_sexo = {\"Homem\":\"Homem\",\"Masculino\":\"Homem\",\"M\":\"Homem\",\n",
    "                 \"Mulher\":\"Mulher\",\"Feminino\":\"Mulher\",\"F\":\"Mulher\"}\n",
    "    df[\"sexo\"] = df[\"sexo\"].map(lambda x: mapa_sexo.get(x, x))\n",
    "\n",
    "# UF / Raça\n",
    "if \"uf\" in df.columns:\n",
    "    df[\"uf\"] = df[\"uf\"].astype(str).str.strip().str.upper()\n",
    "if \"raca_cor\" in df.columns:\n",
    "    df[\"raca_cor\"] = df[\"raca_cor\"].astype(str).str.strip().str.title()\n",
    "\n",
    "# Econômicas\n",
    "for col in [\"trabalhou_semana_passada\", \"teletrabalho\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = to_bool_series(df[col])\n",
    "if \"rendimento\" in df.columns:\n",
    "    df[\"rendimento\"] = pd.to_numeric(df[\"rendimento\"], errors=\"coerce\")\n",
    "if \"situacao_ocupacao\" in df.columns:\n",
    "    df[\"situacao_ocupacao\"] = df[\"situacao_ocupacao\"].astype(str).str.strip().str.title()\n",
    "\n",
    "# Faixas etárias\n",
    "if \"idade\" in df.columns:\n",
    "    bins = [0, 12, 18, 25, 35, 45, 60, 75, np.inf]\n",
    "    labels = [\"0–12\",\"13–18\",\"19–25\",\"26–35\",\"36–45\",\"46–60\",\"61–75\",\"75+\"]\n",
    "    df[\"faixa_etaria\"] = pd.cut(df[\"idade\"], bins=bins, labels=labels, right=True, include_lowest=True)\n",
    "\n",
    "# Sintomas\n",
    "sintomas_cols = [c for c in df.columns if c.startswith(SINTOMA_PREFIX)]\n",
    "for c in sintomas_cols:\n",
    "    df[c] = to_bool_series(df[c])\n",
    "\n",
    "# =========================\n",
    "# BASE CURADA\n",
    "# =========================\n",
    "save_parquet_s3(df, prefix + \"base_curada.parquet\")\n",
    "\n",
    "# =========================\n",
    "# POPULAÇÃO\n",
    "# =========================\n",
    "if \"sexo\" in df.columns:\n",
    "    tab = df[\"sexo\"].value_counts(dropna=False).reset_index()\n",
    "    tab.columns = [\"sexo\",\"qtd\"]\n",
    "    save_parquet_s3(tab, prefix + \"pop_sexo.parquet\")\n",
    "\n",
    "if \"faixa_etaria\" in df.columns:\n",
    "    tab = df[\"faixa_etaria\"].value_counts().sort_index().reset_index()\n",
    "    tab.columns = [\"faixa_etaria\",\"qtd\"]\n",
    "    save_parquet_s3(tab, prefix + \"pop_faixa_etaria.parquet\")\n",
    "\n",
    "if \"raca_cor\" in df.columns:\n",
    "    tab = df[\"raca_cor\"].value_counts(dropna=False).reset_index()\n",
    "    tab.columns = [\"raca_cor\",\"qtd\"]\n",
    "    save_parquet_s3(tab, prefix + \"pop_raca_cor.parquet\")\n",
    "\n",
    "if \"uf\" in df.columns:\n",
    "    top_uf = df[\"uf\"].value_counts().head(10).reset_index()\n",
    "    top_uf.columns = [\"uf\",\"qtd\"]\n",
    "    save_parquet_s3(top_uf, prefix + \"pop_top10_uf.parquet\")\n",
    "\n",
    "if {\"uf\",\"sexo\"}.issubset(df.columns):\n",
    "    tab = df.groupby([\"uf\",\"sexo\"]).size().reset_index(name=\"qtd\")\n",
    "    top_ufs = df[\"uf\"].value_counts().head(10).index\n",
    "    tab = tab[tab[\"uf\"].isin(top_ufs)]\n",
    "    save_parquet_s3(tab, prefix + \"pop_sexo_por_uf.parquet\")\n",
    "\n",
    "# =========================\n",
    "# CLÍNICO – SINTOMAS\n",
    "# =========================\n",
    "if sintomas_cols:\n",
    "    # Prevalência (%) geral por sintoma\n",
    "    prev = (df[sintomas_cols].mean(numeric_only=True) * 100.0).reset_index()\n",
    "    prev.columns = [\"sintoma_raw\",\"perc\"]\n",
    "    prev[\"sintoma\"] = (prev[\"sintoma_raw\"].str.replace(\"^teve_\",\"\",regex=True)\n",
    "                       .str.replace(\"_semana_passada$\",\"\",regex=True)\n",
    "                       .str.replace(\"_\",\" \").str.title())\n",
    "    save_parquet_s3(prev[[\"sintoma\",\"perc\"]], prefix + \"cli_prevalencia_sintomas.parquet\")\n",
    "\n",
    "    # Nº de sintomas por pessoa (severidade)\n",
    "    sint_bool = df[sintomas_cols].astype(\"boolean\")\n",
    "    n_sint = sint_bool.sum(axis=1).value_counts().sort_index().reset_index()\n",
    "    n_sint.columns = [\"n_sintomas\",\"qtd\"]\n",
    "    save_parquet_s3(n_sint, prefix + \"cli_n_sintomas_hist.parquet\")\n",
    "\n",
    "    # Coocorrência (Spearman) – wide\n",
    "    corr = sint_bool.fillna(False).astype(int).corr(method=\"spearman\")\n",
    "    corr_wide = corr.reset_index().rename(columns={\"index\":\"sintoma\"})\n",
    "    save_parquet_s3(corr_wide, prefix + \"cli_coocorrencia_spearman_wide.parquet\")\n",
    "\n",
    "    # Tosse / Febre por sexo (contagem e %)\n",
    "    foco = {\n",
    "        \"tosse\": [c for c in sintomas_cols if \"tosse\" in c],\n",
    "        \"febre\": [c for c in sintomas_cols if \"febre\" in c],\n",
    "    }\n",
    "    if \"sexo\" in df.columns:\n",
    "        base_sexo = df.groupby(\"sexo\").size().rename(\"total\").reset_index()\n",
    "        for nome, cols in foco.items():\n",
    "            if not cols:\n",
    "                continue\n",
    "            any_col = df[cols].any(axis=1)\n",
    "            cont = any_col.groupby(df[\"sexo\"]).sum().reset_index(name=\"qtd\")\n",
    "            save_parquet_s3(cont, prefix + f\"cli_{nome}_contagem_por_sexo.parquet\")\n",
    "            perc = cont.merge(base_sexo, on=\"sexo\")\n",
    "            perc[\"perc\"] = (perc[\"qtd\"] / perc[\"total\"]) * 100.0\n",
    "            save_parquet_s3(perc, prefix + f\"cli_{nome}_percentual_por_sexo.parquet\")\n",
    "\n",
    "# =========================\n",
    "# ECONÔMICO\n",
    "# =========================\n",
    "if \"situacao_ocupacao\" in df.columns:\n",
    "    tab = df[\"situacao_ocupacao\"].value_counts(dropna=False).reset_index()\n",
    "    tab.columns = [\"situacao_ocupacao\",\"qtd\"]\n",
    "    save_parquet_s3(tab, prefix + \"eco_situacao_ocupacao.parquet\")\n",
    "\n",
    "if \"trabalhou_semana_passada\" in df.columns:\n",
    "    tab = df[\"trabalhou_semana_passada\"].value_counts(dropna=False).reset_index()\n",
    "    tab.columns = [\"valor\",\"qtd\"]\n",
    "    save_parquet_s3(tab, prefix + \"eco_trabalhou_semana.parquet\")\n",
    "\n",
    "if \"teletrabalho\" in df.columns:\n",
    "    tab = df[\"teletrabalho\"].value_counts(dropna=False).reset_index()\n",
    "    tab.columns = [\"valor\",\"qtd\"]\n",
    "    save_parquet_s3(tab, prefix + \"eco_teletrabalho.parquet\")\n",
    "\n",
    "if \"rendimento\" in df.columns:\n",
    "    save_parquet_s3(df[[\"rendimento\"]].dropna(), prefix + \"eco_rendimento_base.parquet\")\n",
    "\n",
    "# =========================\n",
    "# TEMPORAL\n",
    "# =========================\n",
    "time_col = next((c for c in [\"mes_pesquisa\",\"semana_mes\"] if c in df.columns), None)\n",
    "\n",
    "if time_col and sintomas_cols:\n",
    "    def any_cols(cols): return df[cols].any(axis=1) if cols else pd.Series([False]*len(df), index=df.index)\n",
    "    febre_cols = [c for c in sintomas_cols if \"febre\" in c]\n",
    "    tosse_cols = [c for c in sintomas_cols if \"tosse\" in c]\n",
    "    cabeca_cols = [c for c in sintomas_cols if \"dor_cabeca\" in c or \"cabeca\" in c]\n",
    "\n",
    "    tmp = df[[time_col]].copy()\n",
    "    tmp[\"sx_febre\"] = any_cols(febre_cols)\n",
    "    tmp[\"sx_tosse\"] = any_cols(tosse_cols)\n",
    "    tmp[\"sx_cabeca\"] = any_cols(cabeca_cols)\n",
    "\n",
    "    trend = (tmp.groupby(time_col)[[\"sx_febre\",\"sx_tosse\",\"sx_cabeca\"]]\n",
    "               .mean()*100.0).reset_index()\n",
    "    trend_long = trend.melt(id_vars=time_col, var_name=\"sintoma\", value_name=\"perc\")\n",
    "    trend_long[\"sintoma\"] = trend_long[\"sintoma\"].str.replace(\"sx_\",\"\",regex=False).str.title()\n",
    "    save_parquet_s3(trend_long, prefix + \"tmp_tendencia_sintomas.parquet\")\n",
    "\n",
    "print(f\"\\n[ETL] Concluído. Entrada via URL e agregados em s3://{bucket_name}/{prefix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1c1de2",
   "metadata": {},
   "source": [
    "### ANALISE DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d330abc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\G'\n",
      "C:\\Users\\dmaradini\\AppData\\Local\\Temp\\ipykernel_18620\\4034816555.py:10: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  OUT_DIR = \"..\\Graficos\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] https://techchallange-637423401077.s3.us-east-1.amazonaws.com/gold/eco_situacao_ocupacao.parquet -> HTTP 403\n",
      "[INFO] Pulando eco_situacao_ocupacao (não encontrado em https://techchallange-637423401077.s3.us-east-1.amazonaws.com/gold/eco_situacao_ocupacao.parquet)\n",
      "[WARN] https://techchallange-637423401077.s3.us-east-1.amazonaws.com/gold/eco_trabalhou_semana.parquet -> HTTP 403\n",
      "[INFO] Pulando eco_trabalhou_semana (não encontrado em https://techchallange-637423401077.s3.us-east-1.amazonaws.com/gold/eco_trabalhou_semana.parquet)\n",
      "[WARN] https://techchallange-637423401077.s3.us-east-1.amazonaws.com/gold/eco_teletrabalho.parquet -> HTTP 403\n",
      "[INFO] Pulando eco_teletrabalho (não encontrado em https://techchallange-637423401077.s3.us-east-1.amazonaws.com/gold/eco_teletrabalho.parquet)\n",
      "[WARN] https://techchallange-637423401077.s3.us-east-1.amazonaws.com/gold/eco_rendimento_base.parquet -> HTTP 403\n",
      "[INFO] Pulando eco_rendimento_base (não encontrado em https://techchallange-637423401077.s3.us-east-1.amazonaws.com/gold/eco_rendimento_base.parquet)\n",
      "fig: ..\\Graficos\\pop_1_sexo.png\n",
      "fig: ..\\Graficos\\pop_2_faixa_etaria.png\n",
      "fig: ..\\Graficos\\pop_3_raca_cor.png\n",
      "fig: ..\\Graficos\\pop_4_top10_uf.png\n",
      "fig: ..\\Graficos\\pop_5_sexo_por_uf.png\n",
      "fig: ..\\Graficos\\cli_1_prevalencia_sintomas.png\n",
      "fig: ..\\Graficos\\cli_2_n_sintomas_hist.png\n",
      "fig: ..\\Graficos\\cli_tosse_contagem_por_sexo.png\n",
      "fig: ..\\Graficos\\cli_tosse_percentual_por_sexo.png\n",
      "fig: ..\\Graficos\\cli_febre_contagem_por_sexo.png\n",
      "fig: ..\\Graficos\\cli_febre_percentual_por_sexo.png\n",
      "fig: ..\\Graficos\\cli_coocorrencia_heatmap.png\n",
      "fig: ..\\Graficos\\tmp_tendencia_sintomas.png\n",
      "Gráficos salvos em: ..\\Graficos\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BASE_URL = \"https://techchallange-637423401077.s3.us-east-1.amazonaws.com/gold/\"\n",
    "\n",
    "OUT_DIR = \"..\\Graficos\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "sns.set_theme(context=\"notebook\", style=\"whitegrid\")\n",
    "sns.set_palette(\"mako\") \n",
    "\n",
    "\n",
    "PARQUETS = {\n",
    "    # POPULAÇÃO\n",
    "    \"pop_sexo\": \"pop_sexo.parquet\",\n",
    "    \"pop_faixa_etaria\": \"pop_faixa_etaria.parquet\",\n",
    "    \"pop_raca_cor\": \"pop_raca_cor.parquet\",\n",
    "    \"pop_top10_uf\": \"pop_top10_uf.parquet\",\n",
    "    \"pop_sexo_por_uf\": \"pop_sexo_por_uf.parquet\",\n",
    "    # CLÍNICO\n",
    "    \"cli_prevalencia_sintomas\": \"cli_prevalencia_sintomas.parquet\",\n",
    "    \"cli_n_sintomas_hist\": \"cli_n_sintomas_hist.parquet\",\n",
    "    \"cli_tosse_contagem_por_sexo\": \"cli_tosse_contagem_por_sexo.parquet\",\n",
    "    \"cli_tosse_percentual_por_sexo\": \"cli_tosse_percentual_por_sexo.parquet\",\n",
    "    \"cli_febre_contagem_por_sexo\": \"cli_febre_contagem_por_sexo.parquet\",\n",
    "    \"cli_febre_percentual_por_sexo\": \"cli_febre_percentual_por_sexo.parquet\",\n",
    "    \"cli_coocorrencia_spearman_wide\": \"cli_coocorrencia_spearman_wide.parquet\",\n",
    "    # ECONÔMICO\n",
    "    \"eco_situacao_ocupacao\": \"eco_situacao_ocupacao.parquet\",\n",
    "    \"eco_trabalhou_semana\": \"eco_trabalhou_semana.parquet\",\n",
    "    \"eco_teletrabalho\": \"eco_teletrabalho.parquet\",\n",
    "    \"eco_rendimento_base\": \"eco_rendimento_base.parquet\",\n",
    "    # TEMPORAL\n",
    "    \"tmp_tendencia_sintomas\": \"tmp_tendencia_sintomas.parquet\",\n",
    "    \"base_curada\": \"base_curada.parquet\",\n",
    "}\n",
    "\n",
    "# ===============================\n",
    "# HELPERS\n",
    "# ===============================\n",
    "def build_url(base: str, fname: str) -> str:\n",
    "    if not base.endswith(\"/\"):\n",
    "        base = base + \"/\"\n",
    "    return base + fname\n",
    "\n",
    "def read_parquet_http(url: str) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Baixa um .parquet via HTTP(S) e devolve um DataFrame.\n",
    "    Retorna None se o arquivo não existir (HTTP 404) ou der erro de rede.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=60)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"[WARN] {url} -> HTTP {r.status_code}\")\n",
    "            return None\n",
    "        bio = BytesIO(r.content)\n",
    "        return pd.read_parquet(bio, engine=\"pyarrow\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] falha ao ler {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def savefig(fig, name):\n",
    "    path = os.path.join(OUT_DIR, name)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, bbox_inches=\"tight\", dpi=120)\n",
    "    plt.close(fig)\n",
    "    print(\"fig:\", path)\n",
    "\n",
    "# ===============================\n",
    "# LEITURA (via URL)\n",
    "# ===============================\n",
    "dfs = {}\n",
    "for k, fname in PARQUETS.items():\n",
    "    url = build_url(BASE_URL, fname)\n",
    "    df = read_parquet_http(url)\n",
    "    if df is None:\n",
    "        print(f\"[INFO] Pulando {k} (não encontrado em {url})\")\n",
    "    else:\n",
    "        dfs[k] = df\n",
    "\n",
    "# ===============================\n",
    "# GRÁFICOS – POPULAÇÃO\n",
    "# ===============================\n",
    "if (tab := dfs.get(\"pop_sexo\")) is not None and not tab.empty:\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    sns.barplot(data=tab.sort_values(\"qtd\"), y=\"sexo\", x=\"qtd\", ax=ax)  # horizontal\n",
    "    ax.set_title(\"Distribuição por Sexo\")\n",
    "    ax.set_xlabel(\"Quantidade\"); ax.set_ylabel(\"Sexo\")\n",
    "    savefig(fig, \"pop_1_sexo.png\")\n",
    "\n",
    "if (tab := dfs.get(\"pop_faixa_etaria\")) is not None and not tab.empty:\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    tab[\"faixa_etaria\"] = tab[\"faixa_etaria\"].astype(str)\n",
    "    sns.barplot(data=tab.sort_values(\"qtd\"), y=\"faixa_etaria\", x=\"qtd\", ax=ax)\n",
    "    ax.set_title(\"Distribuição por Faixa Etária\")\n",
    "    ax.set_xlabel(\"Quantidade\"); ax.set_ylabel(\"Faixa Etária\")\n",
    "    savefig(fig, \"pop_2_faixa_etaria.png\")\n",
    "\n",
    "if (tab := dfs.get(\"pop_raca_cor\")) is not None and not tab.empty:\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    sns.barplot(data=tab.sort_values(\"qtd\"), y=\"raca_cor\", x=\"qtd\", ax=ax)\n",
    "    ax.set_title(\"Distribuição por Raça/Cor\")\n",
    "    ax.set_xlabel(\"Quantidade\"); ax.set_ylabel(\"Raça/Cor\")\n",
    "    savefig(fig, \"pop_3_raca_cor.png\")\n",
    "\n",
    "if (tab := dfs.get(\"pop_top10_uf\")) is not None and not tab.empty:\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    sns.barplot(data=tab.sort_values(\"qtd\"), y=\"uf\", x=\"qtd\", ax=ax)\n",
    "    ax.set_title(\"Top 10 UFs (Contagem)\")\n",
    "    ax.set_xlabel(\"Quantidade\"); ax.set_ylabel(\"UF\")\n",
    "    savefig(fig, \"pop_4_top10_uf.png\")\n",
    "\n",
    "if (tab := dfs.get(\"pop_sexo_por_uf\")) is not None and not tab.empty:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.barplot(data=tab.sort_values(\"qtd\"), y=\"uf\", x=\"qtd\", hue=\"sexo\", ax=ax)\n",
    "    ax.set_title(\"Sexo por UF (Top 10)\")\n",
    "    ax.set_xlabel(\"Quantidade\"); ax.set_ylabel(\"UF\")\n",
    "    ax.legend(title=\"Sexo\")\n",
    "    savefig(fig, \"pop_5_sexo_por_uf.png\")\n",
    "\n",
    "# ===============================\n",
    "# GRÁFICOS – CLÍNICO\n",
    "# ===============================\n",
    "if (tab := dfs.get(\"cli_prevalencia_sintomas\")) is not None and not tab.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    sns.barplot(data=tab.sort_values(\"perc\"), y=\"sintoma\", x=\"perc\", ax=ax)\n",
    "    ax.set_title(\"Prevalência de Sintomas (%) – Geral\")\n",
    "    ax.set_xlabel(\"Percentual (%)\"); ax.set_ylabel(\"Sintoma\")\n",
    "    savefig(fig, \"cli_1_prevalencia_sintomas.png\")\n",
    "\n",
    "if (tab := dfs.get(\"cli_n_sintomas_hist\")) is not None and not tab.empty:\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.barplot(data=tab, y=\"n_sintomas\", x=\"qtd\", ax=ax)  # horizontal\n",
    "    ax.set_title(\"Distribuição do Número de Sintomas\")\n",
    "    ax.set_xlabel(\"Quantidade\"); ax.set_ylabel(\"Nº de Sintomas\")\n",
    "    savefig(fig, \"cli_2_n_sintomas_hist.png\")\n",
    "\n",
    "for nome in [\"tosse\", \"febre\"]:\n",
    "    if (cont := dfs.get(f\"cli_{nome}_contagem_por_sexo\")) is not None and not cont.empty:\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        sns.barplot(data=cont.sort_values(\"qtd\"), y=\"sexo\", x=\"qtd\", ax=ax)\n",
    "        ax.set_title(f\"{nome.title()} – Contagem por Sexo\")\n",
    "        ax.set_xlabel(\"Contagem\"); ax.set_ylabel(\"Sexo\")\n",
    "        savefig(fig, f\"cli_{nome}_contagem_por_sexo.png\")\n",
    "\n",
    "    if (perc := dfs.get(f\"cli_{nome}_percentual_por_sexo\")) is not None and not perc.empty:\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        sns.barplot(data=perc.sort_values(\"perc\"), y=\"sexo\", x=\"perc\", ax=ax)\n",
    "        ax.set_title(f\"{nome.title()} – Percentual por Sexo\")\n",
    "        ax.set_xlabel(\"Percentual (%)\"); ax.set_ylabel(\"Sexo\")\n",
    "        savefig(fig, f\"cli_{nome}_percentual_por_sexo.png\")\n",
    "\n",
    "# Heatmap de coocorrência\n",
    "if (corr_wide := dfs.get(\"cli_coocorrencia_spearman_wide\")) is not None and not corr_wide.empty:\n",
    "    corr = corr_wide.set_index(\"sintoma\")\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(corr, cmap=\"mako\", ax=ax)\n",
    "    ax.set_title(\"Coocorrência de Sintomas (Spearman)\")\n",
    "    savefig(fig, \"cli_coocorrencia_heatmap.png\")\n",
    "\n",
    "# ===============================\n",
    "# GRÁFICOS – ECONÔMICO\n",
    "# ===============================\n",
    "if (tab := dfs.get(\"eco_situacao_ocupacao\")) is not None and not tab.empty:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    sns.barplot(data=tab.sort_values(\"qtd\"), y=\"situacao_ocupacao\", x=\"qtd\", ax=ax)\n",
    "    ax.set_title(\"Situação de Ocupação\")\n",
    "    ax.set_xlabel(\"Quantidade\"); ax.set_ylabel(\"Situação\")\n",
    "    savefig(fig, \"eco_1_situacao_ocupacao.png\")\n",
    "\n",
    "if (tab := dfs.get(\"eco_trabalhou_semana\")) is not None and not tab.empty:\n",
    "    map_lab = {True: \"Sim\", False: \"Não\"}\n",
    "    if \"valor\" in tab.columns:\n",
    "        tab[\"label\"] = tab[\"valor\"].map(map_lab).fillna(\"NA\")\n",
    "    elif \"label\" not in tab.columns:\n",
    "        tab[\"label\"] = tab.iloc[:, 0].map(map_lab).fillna(\"NA\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.barplot(data=tab.sort_values(\"qtd\"), y=\"label\", x=\"qtd\", ax=ax)\n",
    "    ax.set_title(\"Trabalhou na Semana Passada\")\n",
    "    ax.set_xlabel(\"Quantidade\"); ax.set_ylabel(\"\")\n",
    "    savefig(fig, \"eco_2_trabalhou_semana.png\")\n",
    "\n",
    "if (tab := dfs.get(\"eco_teletrabalho\")) is not None and not tab.empty:\n",
    "    map_lab = {True: \"Sim\", False: \"Não\"}\n",
    "    if \"valor\" in tab.columns:\n",
    "        tab[\"label\"] = tab[\"valor\"].map(map_lab).fillna(\"NA\")\n",
    "    elif \"label\" not in tab.columns:\n",
    "        tab[\"label\"] = tab.iloc[:, 0].map(map_lab).fillna(\"NA\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.barplot(data=tab.sort_values(\"qtd\"), y=\"label\", x=\"qtd\", ax=ax)\n",
    "    ax.set_title(\"Teletrabalho\")\n",
    "    ax.set_xlabel(\"Quantidade\"); ax.set_ylabel(\"\")\n",
    "    savefig(fig, \"eco_3_teletrabalho.png\")\n",
    "\n",
    "if (rend := dfs.get(\"eco_rendimento_base\")) is not None and not rend.empty:\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    sns.histplot(data=rend, x=\"rendimento\", bins=30, ax=ax)\n",
    "    ax.set_title(\"Distribuição de Rendimento (R$)\")\n",
    "    ax.set_xlabel(\"R$\"); ax.set_ylabel(\"Frequência\")\n",
    "    savefig(fig, \"eco_4_rendimento_hist.png\")\n",
    "\n",
    "# ===============================\n",
    "# GRÁFICOS – TEMPORAL\n",
    "# ===============================\n",
    "if (trend := dfs.get(\"tmp_tendencia_sintomas\")) is not None and not trend.empty:\n",
    "    # trend deve ter colunas: [<periodo>, \"sintoma\", \"perc\"]\n",
    "    tcol = trend.columns[0] if trend.columns[0] not in [\"sintoma\", \"perc\"] else \"periodo\"\n",
    "    if tcol not in trend.columns:\n",
    "        trend = trend.rename(columns={trend.columns[0]: \"periodo\"})\n",
    "        tcol = \"periodo\"\n",
    "    fig, ax = plt.subplots(figsize=(11, 5))\n",
    "    sns.lineplot(data=trend, x=tcol, y=\"perc\", hue=\"sintoma\", marker=\"o\", ax=ax)\n",
    "    ax.set_title(\"Tendência Temporal de Sintomas (%)\")\n",
    "    ax.set_xlabel(tcol); ax.set_ylabel(\"Percentual (%)\")\n",
    "    savefig(fig, \"tmp_tendencia_sintomas.png\")\n",
    "\n",
    "print(f\"Gráficos salvos em: {OUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
