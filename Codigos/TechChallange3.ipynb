{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d6ca84",
   "metadata": {},
   "source": [
    "#### **Fundamentos de Bancos de Dados Relacionais e NoSQL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59115f",
   "metadata": {},
   "source": [
    "#### **Conteúdo - Bases e Notebook da aula**\n",
    "\n",
    "https://github.com/FIAP/Pos_Tech_DTAT/tree/main/Fase%203"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770d68e",
   "metadata": {},
   "source": [
    "#### **Importação de pacotes, bibliotecas e funções (def)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ccd403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: botocore 1.40.26\n",
      "Uninstalling botocore-1.40.26:\n",
      "  Successfully uninstalled botocore-1.40.26\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall boto3 -y\n",
    "!pip uninstall botocore -y\n",
    "# !pip install boto3==1.40.26 \n",
    "# !pip install botocore==1.40.26\n",
    "# # !pip install s3fs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48bccaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting botocore==1.40.26\n",
      "  Using cached botocore-1.40.26-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\dmaradini\\appdata\\local\\anaconda3\\lib\\site-packages (from botocore==1.40.26) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\dmaradini\\appdata\\local\\anaconda3\\lib\\site-packages (from botocore==1.40.26) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\dmaradini\\appdata\\local\\anaconda3\\lib\\site-packages (from botocore==1.40.26) (2.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dmaradini\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.40.26) (1.16.0)\n",
      "Using cached botocore-1.40.26-py3-none-any.whl (14.0 MB)\n",
      "Installing collected packages: botocore\n",
      "Successfully installed botocore-1.40.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.12.3 requires botocore<1.34.70,>=1.34.41, but you have botocore 1.40.26 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "#!pip install boto3==1.40.26 \n",
    "!pip install botocore==1.40.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "691abc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3: 1.40.26\n",
      "botocore: 1.40.26\n"
     ]
    }
   ],
   "source": [
    "import boto3, botocore\n",
    "print(\"boto3:\", boto3.__version__)\n",
    "print(\"botocore:\", botocore.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad5ca788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar biblioteca completa\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import botocore\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import mongomock\n",
    "import fakeredis\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "# Importar função especifica de um módulo\n",
    "from botocore.exceptions import BotoCoreError, ClientError\n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "from dotenv import load_dotenv\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from astrapy import DataAPIClient\n",
    "# from cassandra.cluster import Cluster\n",
    "# from cassandra.auth import PlainTextAuthProvider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f092b",
   "metadata": {},
   "source": [
    "#### **Testar conexão AWS via Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02baef19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conectado à conta\n",
      "\n",
      "UserId: AROAZI2LFHB2TERRDYNA4:user4377774=maradinidiego16@gmail.com\n",
      "Account: 637423401077\n",
      "Arn: arn:aws:sts::637423401077:assumed-role/voclabs/user4377774=maradinidiego16@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# Validar conexão\n",
    "try:\n",
    "    session = boto3.Session(profile_name=\"default\")\n",
    "    sts = session.client(\"sts\")\n",
    "    identity = sts.get_caller_identity()\n",
    "    print(\"✅ Conectado à conta\\n\")\n",
    "    print(\"UserId:\", identity[\"UserId\"])\n",
    "    print(\"Account:\", identity[\"Account\"])\n",
    "    print(\"Arn:\", identity[\"Arn\"])\n",
    "\n",
    "except (BotoCoreError, ClientError) as e:\n",
    "    print(\"❌ Erro ao conectar à AWS. Verifique suas credenciais e tente novamente.\")\n",
    "    print(\"Detalhes do erro:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d94c24",
   "metadata": {},
   "source": [
    "##### **Conectar ao PostgreSQL via RDS + Executar Comandos SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cc352e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\dmaradini\\AppData\\Local\\Temp\\ipykernel_2944\\2708774549.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  load_dotenv(\"Users\\dmaradini\\.aws\\credentials\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv(\"Users\\dmaradini\\.aws\\credentials\")\n",
    "\n",
    "aws_access_key_id       = os.getenv(\"aws_access_key_id\")\n",
    "aws_secret_access_key   = os.getenv(\"aws_secret_access_key\")\n",
    "aws_session_token       = os.getenv(\"aws_session_token\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ae193b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'techchallange-637423401077' já existe e está acessível.\n",
      "Bucket 'techchallange-637423401077' configurado como público.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json \n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id       = aws_access_key_id,\n",
    "    aws_secret_access_key  = aws_secret_access_key,\n",
    "    aws_session_token      = aws_session_token\n",
    ")\n",
    "\n",
    "bucket_name = \"techchallange-637423401077\"\n",
    "s3_prefix = \"raw\"\n",
    "\n",
    "s3 = session.client('s3')\n",
    "region = s3.meta.region_name or \"us-east-1\"\n",
    "\n",
    "# Criar bucket se não existir\n",
    "try:\n",
    "    s3.head_bucket(Bucket=bucket_name)\n",
    "    print(f\"Bucket '{bucket_name}' já existe e está acessível.\")\n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    if error_code in (\"404\", \"NoSuchBucket\"):\n",
    "        print(f\"Bucket '{bucket_name}' não existe, criando...\\n\")\n",
    "        # Cria bucket público já na criação\n",
    "        if region == \"us-east-1\":\n",
    "            s3.create_bucket(Bucket=bucket_name, ACL='public-read')\n",
    "        else:\n",
    "            s3.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region},\n",
    "                ACL='public-read'\n",
    "            )\n",
    "        print(f\"Bucket '{bucket_name}' criado com sucesso e é público.\\n\")\n",
    "    else:\n",
    "        print(f\"Erro ao acessar o bucket: {e}\")\n",
    "        raise\n",
    "\n",
    "# -----------------------------\n",
    "# Garantir que ACLs públicas não sejam bloqueadas\n",
    "# -----------------------------\n",
    "s3.put_public_access_block(\n",
    "    Bucket=bucket_name,\n",
    "    PublicAccessBlockConfiguration={\n",
    "        'BlockPublicAcls': False,\n",
    "        'IgnorePublicAcls': False,\n",
    "        'BlockPublicPolicy': False,  # mantém políticas públicas bloqueadas (seguro)\n",
    "        'RestrictPublicBuckets': False\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Criar política de bucket pública\n",
    "public_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PublicReadGetObject\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": \"*\",\n",
    "            \"Action\": \"s3:GetObject\",\n",
    "            \"Resource\": f\"arn:aws:s3:::{bucket_name}/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(public_policy))\n",
    "print(f\"Bucket '{bucket_name}' configurado como público.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00071434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexão estabelecida.\n",
      "Bucket 'techchallange-637423401077' já existe e está acessível.\n",
      "Bucket 'techchallange-637423401077' configurado como público.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import numpy as npfrp\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id       =   aws_access_key_id\n",
    "    ,aws_secret_access_key  =   aws_secret_access_key\n",
    "    ,aws_session_token      =   aws_session_token\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "bucket_name = \"techchallange-637423401077\"\n",
    "s3_prefix = \"raw\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Conexão estabelecida.\")\n",
    "\n",
    "s3 = session.client('s3')\n",
    "region = s3.meta.region_name or \"us-east-1\"\n",
    "\n",
    "try:\n",
    "    s3.head_bucket(Bucket=bucket_name)\n",
    "    print(f\"Bucket '{bucket_name}' já existe e está acessível.\")\n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    if error_code in (\"404\", \"NoSuchBucket\"):\n",
    "        print(f\"Bucket '{bucket_name}' não existe, criando...\\n\")\n",
    "        if region == \"us-east-1\":\n",
    "            s3.create_bucket(Bucket=bucket_name)\n",
    "            \n",
    "        else:\n",
    "            s3.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f\"Bucket '{bucket_name}' criado com sucesso.\\n\")\n",
    "    else:\n",
    "        print(f\"Erro ao acessar o bucket: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "# Criar política de bucket pública\n",
    "public_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PublicReadGetObject\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": \"*\",\n",
    "            \"Action\": \"s3:GetObject\",\n",
    "            \"Resource\": f\"arn:aws:s3:::{bucket_name}/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(public_policy))\n",
    "print(f\"Bucket '{bucket_name}' configurado como público.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2254431",
   "metadata": {},
   "source": [
    "### CAMADA BRONZE (EXTRACT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28b14473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando PNAD_COVID_052020...\n",
      "Dados salvos no S3 em s3://techchallange-637423401077/raw/PNAD_COVID_052020.csv\n",
      "Processando PNAD_COVID_062020...\n",
      "Dados salvos no S3 em s3://techchallange-637423401077/raw/PNAD_COVID_062020.csv\n",
      "Processando PNAD_COVID_072020...\n",
      "Dados salvos no S3 em s3://techchallange-637423401077/raw/PNAD_COVID_072020.csv\n",
      "Exportação concluída com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from io import BytesIO, StringIO\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "\n",
    "bucket_name = \"techchallange-637423401077\"\n",
    "s3_prefix = \"raw\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# URLs dos arquivos do IBGE\n",
    "urls = {\n",
    "    \"PNAD_COVID_052020\": \"https://ftp.ibge.gov.br/Trabalho_e_Rendimento/Pesquisa_Nacional_por_Amostra_de_Domicilios_PNAD_COVID19/Microdados/Dados/PNAD_COVID_052020.zip\",\n",
    "    \"PNAD_COVID_062020\": \"https://ftp.ibge.gov.br/Trabalho_e_Rendimento/Pesquisa_Nacional_por_Amostra_de_Domicilios_PNAD_COVID19/Microdados/Dados/PNAD_COVID_062020.zip\",\n",
    "    \"PNAD_COVID_072020\": \"https://ftp.ibge.gov.br/Trabalho_e_Rendimento/Pesquisa_Nacional_por_Amostra_de_Domicilios_PNAD_COVID19/Microdados/Dados/PNAD_COVID_072020.zip\",\n",
    "}\n",
    "\n",
    "for tabela, url in urls.items():\n",
    "    print(f\"Processando {tabela}...\")\n",
    "\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    if \"Dicionario\" in tabela:\n",
    "        # Salvar o XLS original no S3\n",
    "        s3_key = f\"{s3_prefix}/{tabela}.xls\"\n",
    "        s3.put_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=s3_key,\n",
    "            Body=r.content  # conteúdo binário do XLS\n",
    "        )\n",
    "        print(f\"Dicionário salvo no S3 em s3://{bucket_name}/{s3_key}\")\n",
    "\n",
    "    else:\n",
    "        # Abrir o ZIP e extrair o CSV\n",
    "        with ZipFile(BytesIO(r.content)) as z:\n",
    "            csv_name = z.namelist()[0]\n",
    "            with z.open(csv_name) as f:\n",
    "                df = pd.read_csv(f, sep=\";\", encoding=\"latin1\")\n",
    "\n",
    "        # Converter para CSV em memória\n",
    "        csv_buffer = StringIO()\n",
    "        df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "        s3_key = f\"{s3_prefix}/{tabela}.csv\"\n",
    "        s3.put_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=s3_key,\n",
    "            Body=csv_buffer.getvalue()\n",
    "        )\n",
    "        print(f\"Dados salvos no S3 em s3://{bucket_name}/{s3_key}\")\n",
    "\n",
    "print(\"Exportação concluída com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a90a04a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consegui conectar no S3!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name=\"default\")\n",
    "s3 = session.client(\"s3\")\n",
    "print(\"Consegui conectar no S3!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdffda6",
   "metadata": {},
   "source": [
    "### CAMADA SILVER (TRANFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca9b2f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo https://techchallange-637423401077.s3.us-east-1.amazonaws.com/raw/PNAD_COVID_052020.csv\n",
      "Lendo https://techchallange-637423401077.s3.us-east-1.amazonaws.com/raw/PNAD_COVID_062020.csv\n",
      "Lendo https://techchallange-637423401077.s3.us-east-1.amazonaws.com/raw/PNAD_COVID_072020.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Lista de arquivos CSV no bucket público\n",
    "bronze_files = [\n",
    "    \"PNAD_COVID_052020.csv\",\n",
    "    \"PNAD_COVID_062020.csv\",\n",
    "    \"PNAD_COVID_072020.csv\"\n",
    "]\n",
    "\n",
    "# URL base do bucket público\n",
    "base_url = \"https://techchallange-637423401077.s3.us-east-1.amazonaws.com/raw/\"\n",
    "\n",
    "lista_de_dataframes = []\n",
    "\n",
    "for file_name in bronze_files:\n",
    "    file_url = f\"{base_url}{file_name}\"\n",
    "    print(f\"Lendo {file_url}\")\n",
    "    \n",
    "    # Faz download do CSV como texto\n",
    "    response = requests.get(file_url)\n",
    "    response.raise_for_status()  # garante que deu certo\n",
    "    \n",
    "    # Lê o CSV como texto cru\n",
    "    df_raw = pd.read_csv(StringIO(response.text), header=None)\n",
    "    \n",
    "    # Divide a única coluna pelo separador vírgula\n",
    "    df_temp = df_raw[0].str.split(\",\", expand=True)\n",
    "    \n",
    "    # Define a primeira linha como header\n",
    "    df_temp.columns = df_temp.iloc[0]\n",
    "    df_temp = df_temp.drop(0).reset_index(drop=True)\n",
    "    \n",
    "    lista_de_dataframes.append(df_temp)\n",
    "\n",
    "# Consolidar todos os dataframes\n",
    "df_consolidado = pd.concat(lista_de_dataframes, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "039dae51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame salvo em s3://techchallange-637423401077/silver/df_consolidado.parquet\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# Configurações do bucket\n",
    "bucket_name = \"techchallange-637423401077\"\n",
    "s3_key = \"silver/df_consolidado.parquet\"\n",
    "\n",
    "# Conexão S3\n",
    "s3_client = boto3.client(\"s3\")  # precisa das credenciais configuradas no seu ambiente\n",
    "\n",
    "# Salva o DataFrame em um buffer Parquet\n",
    "buffer = BytesIO()\n",
    "df_consolidado.to_parquet(buffer, index=False)\n",
    "\n",
    "# Envia para o S3\n",
    "buffer.seek(0)\n",
    "s3_client.put_object(Bucket=bucket_name, Key=s3_key, Body=buffer.getvalue())\n",
    "\n",
    "print(f\"DataFrame salvo em s3://{bucket_name}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db6a0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consolidado_parquet_aws = pd.read_parquet(\"https://techchallange-637423401077.s3.us-east-1.amazonaws.com/silver/df_consolidado.parquet\")\n",
    "#pd.read_parquet('Dados.parquet') #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff64cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consolidado_parquet = df_consolidado_parquet_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7776874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dicionário de renomeação completo\n",
    "mapa_colunas = {\n",
    "    # Colunas fixas\n",
    "    \"ANO\": \"ano\",\n",
    "    \"V1013\": \"mes_pesquisa\",\n",
    "    \"V1012\": \"semana_mes\",\n",
    "    \"UF\": \"uf\",\n",
    "    \"CAPITAL\": \"capital\",\n",
    "\n",
    "    # Colunas desejadas\n",
    "    \"A002\": \"idade\",\n",
    "    \"A003\": \"sexo\",\n",
    "    \"A004\": \"raca_cor\",\n",
    "    \"A006B\": \"aulas_presenciais\",\n",
    "    \"B008\": \"fez_exame\",\n",
    "    \"B009A\": \"exame_swab\",\n",
    "    \"B009C\": \"exame_furo_dedo\",\n",
    "    \"B009E\": \"exame_veia_braco\",\n",
    "    \"A005\": \"escolaridade\",\n",
    "    \"A006\": \"frequenta_escola\",\n",
    "    \"A006A\": \"tipo_escola\",\n",
    "    \"B0011\": \"teve_febre_semana_passada\",\n",
    "    \"B0012\": \"teve_tosse_semana_passada\",\n",
    "    \"B0013\": \"teve_dor_garganta_semana_passada\",\n",
    "    \"B0014\": \"teve_dificuldade_respirar\",\n",
    "    \"B0015\": \"teve_dor_cabeca\",\n",
    "    \"B0016\": \"teve_dor_peito\",\n",
    "    \"B0017\": \"teve_nausea\",\n",
    "    \"B0018\": \"teve_nariz_entupido\",\n",
    "    \"B0019\": \"teve_fadiga\",\n",
    "    \"B00110\": \"teve_dor_olhos\",\n",
    "    \"B00111\": \"teve_perda_cheiro_sabor\",\n",
    "    \"B00112\": \"teve_dor_muscular\",\n",
    "    \"B00113\": \"teve_diarreia\"\n",
    "}\n",
    "\n",
    "\n",
    "colunas_existentes = [col for col in mapa_colunas.keys() if col in df_consolidado_parquet.columns]\n",
    "\n",
    "# Renomeia apenas essas colunas\n",
    "df_consolidado_parquet.rename(columns={col: mapa_colunas[col] for col in colunas_existentes}, inplace=True)\n",
    "\n",
    "\n",
    "df_consolidado_parquet = df_consolidado_parquet[[mapa_colunas[col] for col in colunas_existentes]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa_sim_nao = {\n",
    "    1: \"Sim\",\n",
    "    2: \"Não\",\n",
    "    3: \"Não sabe\",\n",
    "    9: \"Ignorado\"\n",
    "}\n",
    "\n",
    "mapa_uf = {\n",
    "    11: \"Rondônia\", 12: \"Acre\", 13: \"Amazonas\", 14: \"Roraima\", 15: \"Pará\", 16: \"Amapá\", 17: \"Tocantins\",\n",
    "    21: \"Maranhão\", 22: \"Piauí\", 23: \"Ceará\", 24: \"Rio Grande do Norte\", 25: \"Paraíba\", 26: \"Pernambuco\",\n",
    "    27: \"Alagoas\", 28: \"Sergipe\", 29: \"Bahia\", 31: \"Minas Gerais\", 32: \"Espírito Santo\", 33: \"Rio de Janeiro\",\n",
    "    35: \"São Paulo\", 41: \"Paraná\", 42: \"Santa Catarina\", 43: \"Rio Grande do Sul\", 50: \"Mato Grosso do Sul\",\n",
    "    51: \"Mato Grosso\", 52: \"Goiás\", 53: \"Distrito Federal\"\n",
    "}\n",
    "\n",
    "\n",
    "df_consolidado_parquet = df_consolidado_parquet.copy()\n",
    "\n",
    "df_consolidado_parquet[\"uf\"] = df_consolidado_parquet[\"uf\"].astype(int)\n",
    "df_consolidado_parquet[\"uf\"] = df_consolidado_parquet[\"uf\"].replace(mapa_uf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Substituir colunas de Sim/Não (exemplo: febre, tosse, exames, etc)\n",
    "colunas_sim_nao = [\n",
    "    \"fez_exame\", \"exame_swab\", \"exame_furo_dedo\", \"exame_veia_braco\",\n",
    "    \"teve_febre_semana_passada\", \"teve_tosse_semana_passada\", \"teve_dor_garganta_semana_passada\",\n",
    "    \"teve_dificuldade_respirar\", \"teve_dor_cabeca\", \"teve_dor_peito\", \"teve_nausea\",\n",
    "    \"teve_nariz_entupido\", \"teve_fadiga\", \"teve_dor_olhos\", \"teve_perda_cheiro_sabor\",\n",
    "    \"teve_dor_muscular\", \"teve_diarreia\", \"aulas_presenciais\", \"frequenta_escola\"\n",
    "]\n",
    "colunas_sim_nao = [\n",
    "    \"fez_exame\", \"exame_swab\", \"exame_furo_dedo\", \"exame_veia_braco\",\n",
    "    \"teve_febre_semana_passada\", \"teve_tosse_semana_passada\", \"teve_dor_garganta_semana_passada\",\n",
    "    \"teve_dificuldade_respirar\", \"teve_dor_cabeca\", \"teve_dor_peito\", \"teve_nausea\",\n",
    "    \"teve_nariz_entupido\", \"teve_fadiga\", \"teve_dor_olhos\", \"teve_perda_cheiro_sabor\",\n",
    "    \"teve_dor_muscular\", \"teve_diarreia\", \"aulas_presenciais\", \"frequenta_escola\"\n",
    "]\n",
    "\n",
    "# Criar um dicionário de substituição com strings\n",
    "mapa_sim_nao_str = {str(k): v for k, v in mapa_sim_nao.items()}\n",
    "\n",
    "for col in colunas_sim_nao:\n",
    "    if col in df_consolidado_parquet.columns:\n",
    "        # Substituir valores apenas se não forem nulos\n",
    "        df_consolidado_parquet[col] = df_consolidado_parquet[col].apply(\n",
    "            lambda x: mapa_sim_nao_str.get(str(x), x) if pd.notna(x) else x\n",
    "        )\n",
    "\n",
    "# Dicionário de substituição\n",
    "mapa_sexo = {\n",
    "    \"1\": \"Homem\",\n",
    "    \"2\": \"Mulher\"\n",
    "}\n",
    "\n",
    "# Substituir valores na coluna 'sexo' (somente se a coluna existir)\n",
    "if \"sexo\" in df_consolidado_parquet.columns:\n",
    "    df_consolidado_parquet[\"sexo\"] = df_consolidado_parquet[\"sexo\"].astype(str).replace(mapa_sexo)\n",
    "\n",
    "\n",
    "# Dicionário de mapeamento para raca_cor\n",
    "mapa_raca_cor = {\n",
    "    \"1\": \"Branca\",\n",
    "    \"2\": \"Preta\",\n",
    "    \"3\": \"Amarela\",\n",
    "    \"4\": \"Parda\",\n",
    "    \"5\": \"Indígena\",\n",
    "    \"9\": \"Ignorado\"\n",
    "}\n",
    "\n",
    "# Aplicar mapeamento no dataframe\n",
    "if \"raca_cor\" in df_consolidado_parquet.columns:\n",
    "    df_consolidado_parquet[\"raca_cor\"] = df_consolidado_parquet[\"raca_cor\"].astype(str).replace(mapa_raca_cor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11897196",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consolidado_parquet[\"sexo\"] = df_consolidado_parquet[\"sexo\"].astype(\"category\")\n",
    "df_consolidado_parquet[\"mes_pesquisa\"] = df_consolidado_parquet[\"mes_pesquisa\"].astype(\"category\")\n",
    "df_consolidado_parquet[\"raca_cor\"] = df_consolidado_parquet[\"raca_cor\"].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47886e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame salvo em s3://techchallange-637423401077/silver/df_consolidado_Tratado.parquet\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# Seu DataFrame\n",
    "# df_consolidado\n",
    "\n",
    "# Configurações do bucket\n",
    "bucket_name = \"techchallange-637423401077\"\n",
    "s3_key = \"silver/df_consolidado_Tratado.parquet\"\n",
    "\n",
    "# Conexão S3\n",
    "s3_client = boto3.client(\"s3\")  # precisa das credenciais configuradas no seu ambiente\n",
    "\n",
    "# Salva o DataFrame em um buffer Parquet\n",
    "buffer = BytesIO()\n",
    "df_consolidado_parquet.to_parquet(buffer, index=False)\n",
    "\n",
    "# Envia para o S3\n",
    "buffer.seek(0)\n",
    "s3_client.put_object(Bucket=bucket_name, Key=s3_key, Body=buffer.getvalue())\n",
    "\n",
    "print(f\"DataFrame salvo em s3://{bucket_name}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eb1505",
   "metadata": {},
   "source": [
    "### CAMADA OURO (LOAD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c47b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter sintomas \"Sim\"/\"Não\" em 1/0\n",
    "sintomas = [ 'teve_febre_semana_passada', 'teve_tosse_semana_passada',\n",
    "       'teve_dor_garganta_semana_passada', 'teve_dificuldade_respirar',\n",
    "       'teve_dor_cabeca', 'teve_dor_peito', 'teve_nausea',\n",
    "       'teve_nariz_entupido', 'teve_fadiga', 'teve_dor_olhos',\n",
    "       'teve_perda_cheiro_sabor', 'teve_dor_muscular', 'teve_diarreia']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0de405fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_consolidado_parquet.copy()\n",
    "\n",
    "# df = df.groupby(\"raca_cor\")[sintomas].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6995fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmaradini\\AppData\\Local\\Temp\\ipykernel_2944\\2682411368.py:7: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df_sexo_agrupado = df_sexo.groupby(\"sexo\")[sintomas].sum().reset_index()\n"
     ]
    }
   ],
   "source": [
    "df_sexo = df_consolidado_parquet.copy()\n",
    "\n",
    "for col in sintomas:\n",
    "    df_sexo[col] = df_sexo[col].map({\"Sim\": 1, \"Não\": 0})\n",
    "\n",
    "# Agrupar por sexo\n",
    "df_sexo_agrupado = df_sexo.groupby(\"sexo\")[sintomas].sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d73599ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmaradini\\AppData\\Local\\Temp\\ipykernel_2944\\2795615387.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df_mes_agrupado = df_mes.groupby(\"mes_pesquisa\")[sintomas].sum().reset_index()\n"
     ]
    }
   ],
   "source": [
    "df_mes = df_consolidado_parquet.copy()\n",
    "for col in sintomas:\n",
    "    df_mes[col] = df_mes[col].map({\"Sim\": 1, \"Não\": 0})\n",
    "\n",
    "df_mes_agrupado = df_mes.groupby(\"mes_pesquisa\")[sintomas].sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b755b5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmaradini\\AppData\\Local\\Temp\\ipykernel_2944\\3495711940.py:6: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df_sexo_mes_agrupado = df_sexo_mes.groupby([\"mes_pesquisa\", \"sexo\"])[sintomas].sum().reset_index()\n"
     ]
    }
   ],
   "source": [
    "df_sexo_mes = df_consolidado_parquet.copy()\n",
    "for col in sintomas:\n",
    "    df_sexo_mes[col] = df_sexo_mes[col].map({\"Sim\": 1, \"Não\": 0})\n",
    "\n",
    "# Agrupar por sexo e mês\n",
    "df_sexo_mes_agrupado = df_sexo_mes.groupby([\"mes_pesquisa\", \"sexo\"])[sintomas].sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02919838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame salvo em s3://techchallange-637423401077/gold/df_sexo_agrupado.parquet\n",
      "DataFrame salvo em s3://techchallange-637423401077/gold/df_mes_agrupado.parquet\n",
      "DataFrame salvo em s3://techchallange-637423401077/gold/df_sexo_mes_agrupado.parquet\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import boto3\n",
    "\n",
    "# Cliente S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "bucket_name = \"techchallange-637423401077\"  # só o bucket\n",
    "\n",
    "# Dicionário com nomes e dataframes\n",
    "dataframes = {\n",
    "    \"df_sexo_agrupado.parquet\": df_sexo_agrupado,\n",
    "    \"df_mes_agrupado.parquet\": df_mes_agrupado,\n",
    "    \"df_sexo_mes_agrupado.parquet\": df_sexo_mes_agrupado\n",
    "}\n",
    "\n",
    "# Prefixo (pasta) dentro do bucket\n",
    "prefix = \"gold/\"\n",
    "\n",
    "# Loop para salvar todos no S3\n",
    "for file_name, df in dataframes.items():\n",
    "    buffer = BytesIO()\n",
    "    df.to_parquet(buffer, index=False)\n",
    "    buffer.seek(0)\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=prefix + file_name, Body=buffer.getvalue())\n",
    "    print(f\"DataFrame salvo em s3://{bucket_name}/{prefix}{file_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
